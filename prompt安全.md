# Prompt注入攻击
定义： 输入恶意prompt内容，让LLM违背开发者预设的指令，输出恶意内容

## 直接攻击手法
+ 手动
  - 前缀、后缀注入： 无害前缀后缀改变上下文信息（大模型无法反悔）
  - 拒绝抑制、风格抑制： 让大模型无法否定，限制大模型的行为，逼迫大模型犯错
  - 虚构场景： 打破对话场景，要求模型..
  - 编码绕过：大模型泛化能力有限
  - 注意力转移：模型专注于其他目标，忘记安全目标；要求模型续写文本、进行逻辑推理、文本翻译、模拟程序运行
+ 自动
  - 通过梯度计算+优化算法，逐比特优化prompt，生成可以达到攻击目的的恶意prompt
  - 用于攻击和分析自然语言处理的通用对抗触发器（UAT）

## 间接攻击手法
攻击者把恶意输入注入到第三方资料中，一旦大模型某个时间获取第三方资料作为prompt，就会被攻击
  
### 攻击场景
端侧大模型截图转文字，评论区被注入恶意内容，转文字后作为prompt的一部分输入给大模型，造成prompt注入

# Prompt越狱
违背模型的安全预设，例如模型训练时的安全对齐

# 防御

## 输入端防御
+ 困惑审查
+ 输入替换： 通过改写或净化来消除prompt中的恶意意图
+ 输入转换（翻译成中间语言或其他语言）来识别恶意prompt
+ 输入过滤： 拒绝识别成恶意prompt的输入

## 输出端防御
+ 输出过滤： 检测恶意输出（通过打分/分类机制或其他的LLM）
+ 输出重复：让大模型重复输出相同prompt回复，如果无法重复，说明可能存在攻击行为

## 专用模型检测恶意prompt

# 案例
+ 谷歌Model Armor过滤prompt输入：类似Web安全中的WAF
+ Meta的Purple Llama防御体系
+ 攻击demo：https://prompting.ai.immersivelabs.com/
